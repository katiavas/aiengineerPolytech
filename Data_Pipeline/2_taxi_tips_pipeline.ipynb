{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69689bb-9845-4ca9-8793-d1d98cfc4f7e",
   "metadata": {},
   "source": [
    "# Taxi tips Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using what you learnt in the 2 last notebooks, you will create a pipeline with 3 components to load, train and test a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786076c6-c86f-456f-af2c-40fa6d78014c",
   "metadata": {},
   "source": [
    "### create all components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to store the components definitions\n",
    "!mkdir components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da00fc4-c61e-4ea3-9e57-2b6b9599d82f",
   "metadata": {},
   "source": [
    "#### Read data on a minio bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbb82f-1dcf-4c88-8728-96ce9763b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp as kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "import os\n",
    "from kfp.components import InputPath, OutputPath, create_component_from_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da3865-2c21-45d9-8a8b-40e6895debb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_minio(\n",
    "    minio_path: str,\n",
    "    bucket: str,\n",
    "    dest_file_path: OutputPath(),\n",
    "    ):\n",
    "    \n",
    "    import numpy\n",
    "    from io import BytesIO\n",
    "    import pandas as pd\n",
    "    import urllib3\n",
    "    from minio import Minio\n",
    "    import os\n",
    "    import pyarrow\n",
    "\n",
    "    client = Minio(\n",
    "    ...\n",
    "    )\n",
    "\n",
    "    # Get data from minio using get_object, decode it using BytesIO and read the parquet result with pandas\n",
    "    try:\n",
    "        ...\n",
    "    finally:\n",
    "        response.close()\n",
    "        response.release_conn()\n",
    "    ### pass dataset to component output\n",
    "    data.to_parquet(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b5a3c-ac29-4ace-a53b-7b4eba59194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_component_from_func(\n",
    "    ...,\n",
    "    output_component_file='components/get_data_from_minio.yaml',\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=[\n",
    "        'numpy==1.21.6',\n",
    "        'minio==6.0.2',\n",
    "        'pandas==1.0.5',\n",
    "        'pyarrow==10.0.1'\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742d0ee-2535-4004-91e7-71ad662764d2",
   "metadata": {},
   "source": [
    "#### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444388eb-a5c0-4d9c-aec6-51b01e4f6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(\n",
    "    training_data_path: InputPath('CSV'),  # Also supports LibSVM\n",
    "    model_path: OutputPath('XGBoostModel'),\n",
    "    model_config_path: OutputPath('XGBoostModelConfig'),\n",
    "    training_log_path: OutputPath(),\n",
    "    starting_model_path: InputPath('XGBoostModel') = None,\n",
    "    \n",
    "\n",
    "\n",
    "    label_column: int = 0,\n",
    "    num_iterations: int = 10,\n",
    "    booster_params: dict = None,\n",
    "\n",
    "    # Booster parameters\n",
    "    objective: str = 'reg:squarederror',\n",
    "    booster: str = 'gbtree',\n",
    "    learning_rate: float = 0.3,\n",
    "    min_split_loss: float = 0,\n",
    "    max_depth: int = 6,\n",
    "):\n",
    "    '''Train an XGBoost model.\n",
    "    Args:\n",
    "        training_data_path: Path for the training data in CSV format.\n",
    "        model_path: Output path for the trained model in binary XGBoost format.\n",
    "        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.\n",
    "        starting_model_path: Path for the existing trained model to start from.\n",
    "        label_column: Column containing the label data.\n",
    "        num_boost_rounds: Number of boosting iterations.\n",
    "        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "        objective: The learning task and the corresponding learning objective.\n",
    "            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "            The most common values are:\n",
    "            \"reg:squarederror\" - Regression with squared loss (default).\n",
    "            \"reg:logistic\" - Logistic regression.\n",
    "            \"binary:logistic\" - Logistic regression for binary classification, output probability.\n",
    "            \"binary:logitraw\" - Logistic regression for binary classification, output score before logistic transformation\n",
    "            \"rank:pairwise\" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n",
    "            \"rank:ndcg\" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n",
    "\n",
    "    '''\n",
    "    import pandas\n",
    "    import xgboost\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from tensorboardX import SummaryWriter\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pyarrow\n",
    "    \n",
    "    \n",
    "    ### embedded function to allow tensorboard to monitor the training ###\n",
    "    def TensorBoardCallback():\n",
    "        writer = SummaryWriter(training_log_path)\n",
    "\n",
    "        def callback(env):\n",
    "            for k, v in env.evaluation_result_list:\n",
    "                print(k,v)\n",
    "                writer.add_scalar(k, v, env.iteration)\n",
    "\n",
    "        return callback\n",
    "    \n",
    "    ### load data ###\n",
    "    \n",
    "    df = ...\n",
    "    \n",
    "    ### autoclean data to allow only copatible types in features\n",
    "    numerics = ['int','float']\n",
    "    df = df.select_dtypes(...)\n",
    "    \n",
    "    ### split data ###\n",
    "\n",
    "    data=...\n",
    "    label=...\n",
    "    X_train, X_test, y_train, y_test = ...\n",
    "    \n",
    "    dtrain = ...\n",
    "    dtest = ...\n",
    "\n",
    "    ### model HP ###\n",
    "\n",
    "    booster_params = booster_params or {}\n",
    "    booster_params.setdefault('objective', objective)\n",
    "    booster_params.setdefault('booster', booster)\n",
    "    booster_params.setdefault('learning_rate', learning_rate)\n",
    "    booster_params.setdefault('min_split_loss', min_split_loss)\n",
    "    booster_params.setdefault('max_depth', max_depth)\n",
    "    \n",
    "    ### Not from scratch training management ###\n",
    "\n",
    "    starting_model = None\n",
    "    if starting_model_path:\n",
    "        starting_model = xgboost.Booster(model_file=starting_model_path)\n",
    "\n",
    "    ### Model fit to data ###\n",
    "\n",
    "    model = xgboost.train(\n",
    "        ...\n",
    "    )\n",
    "        \n",
    "    ### Save the model as an artifact ###\n",
    "    model.save_model(...)\n",
    "\n",
    "    # save the model config\n",
    "    model_config_str = model.save_config()\n",
    "    with open(model_config_path, 'w') as model_config_file:\n",
    "        model_config_file.write(...)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926bfc50-7d69-428c-a396-b854679367f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_component_from_func(\n",
    "    ...,\n",
    "    output_component_file='components/xgb_train_dbg.yaml',\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=[\n",
    "        'xgboost==1.1.1',\n",
    "        'pandas==1.0.5',\n",
    "        'tensorboardX==2.5.1',\n",
    "        'scikit-learn==1.0',\n",
    "        'pyarrow==10.0.1'\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28483883-47c8-47ed-89b0-b381c97e62ac",
   "metadata": {},
   "source": [
    "#### Make predictions with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff0d27-417a-4bd7-b13d-9dbd2195daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgboost_predict(\n",
    "    data_path: InputPath('CSV'),  # Also supports LibSVM\n",
    "    model_path: InputPath('XGBoostModel'),\n",
    "    predictions_path: OutputPath('Predictions'),\n",
    "    label_column: int = None,\n",
    "):\n",
    "    '''Make predictions using a trained XGBoost model.\n",
    "    Args:\n",
    "        data_path: Path for the feature data in CSV format.\n",
    "        model_path: Path for the trained model in binary XGBoost format.\n",
    "        predictions_path: Output path for the predictions.\n",
    "        label_column: Column containing the label data.\n",
    "    '''\n",
    "    from pathlib import Path\n",
    "\n",
    "    import numpy\n",
    "    import pandas\n",
    "    import xgboost\n",
    "    import pyarrow\n",
    "\n",
    "    df = pandas.read_parquet(\n",
    "        data_path,\n",
    "    )\n",
    "    \n",
    "    ### autoclean data to allow only copatible types in features\n",
    "    numerics = ['int','float']\n",
    "    df = ...\n",
    "\n",
    "    ### drop label column if provided\n",
    "    if ... :\n",
    "        df = ...\n",
    "\n",
    "    testing_data = xgboost.DMatrix(\n",
    "        data=df,\n",
    "    )\n",
    "\n",
    "    model = xgboost.Booster(model_file=model_path)\n",
    "\n",
    "    predictions = model.predict(testing_data)\n",
    "\n",
    "    Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    numpy.savetxt(predictions_path, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e94b53-4700-45bf-bc7b-0340c37c5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_component_from_func(\n",
    "    ...,\n",
    "    output_component_file='components/xgb_predict.yaml',\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=[\n",
    "        'xgboost==1.1.1',\n",
    "        'pandas==1.0.5',\n",
    "        'pyarrow==10.0.1'\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9354e8-41d1-44b3-83b2-b779bb3e8a90",
   "metadata": {},
   "source": [
    "### Load components to use them in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c179035-3649-4de1-b2b6-f524a8c45479",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_from_minio_op = components.load_component_from_file(...)\n",
    "xgboost_predict_on_csv_op=components.load_component_from_file(...)\n",
    "xgboost_train_on_csv_op=components.load_component_from_file(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe92fc4-fc97-4ba2-b145-fc096fa78fe3",
   "metadata": {},
   "source": [
    "### Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea58880-cdc8-48e7-83bc-461d4349c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=''#firstname-lastname\n",
    "namespace = f'kubeflow-user-{user}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9659981-17b0-4b34-8344-a227385f9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='xgboost_performance')\n",
    "def xgboost_pipeline(namespace=namespace):\n",
    "    import datetime\n",
    "    from kfp.onprem import use_k8s_secret\n",
    "    \n",
    "    bucket=''#firstname-lastname\n",
    "    \n",
    "    data = get_data_from_minio_op(\n",
    "        minio_path = 'datasets/chicago/trips.parquet',\n",
    "        bucket = bucket,\n",
    "    )\n",
    "    \n",
    "    ### this allows using real secret in a component\n",
    "    data.apply(\n",
    "        use_k8s_secret(\n",
    "            secret_name='minio-service-account',\n",
    "            k8s_secret_key_to_env={\n",
    "                'access_key':'MINIO_ACCESS_KEY',\n",
    "                'secret_key':'MINIO_SECRET_KEY'\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Training and prediction on dataset in CSV format\n",
    "    model_trained_on_csv = xgboost_train_on_csv_op(\n",
    "        training_data=data.output,\n",
    "        label_column=0,\n",
    "        objective='reg:squarederror',\n",
    "        num_iterations=200,\n",
    "    ).set_memory_limit('1Gi').outputs\n",
    "    \n",
    "    xgboost_predict_on_csv_op(\n",
    "        data=data.output,\n",
    "        model=model_trained_on_csv['model'],\n",
    "        label_column=0,\n",
    "    ).set_memory_limit('1Gi')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bed237-17f1-49f4-bad2-167925bfc5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64abd3-98bf-4eb8-9872-c50b5ce22a03",
   "metadata": {},
   "source": [
    "### Create the KFP client to link with Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601abdec-e330-46f5-99f9-be8419aeedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### a token has been automatically provided in the KF_PIPELINES_SA_TOKEN_PATH variable. This token allow acc√®s to only your namespace\n",
    "token_file = os.getenv(\"KF_PIPELINES_SA_TOKEN_PATH\")\n",
    "with open(token_file) as f:\n",
    "    token = f.readline()\n",
    "client = kfp.Client(host='http://ml-pipeline.kubeflow.svc.cluster.local:8888',\n",
    "               existing_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943da61-d683-4833-8f9a-1aafd6cb7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Aiengineer labs session2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e777033-bb44-4b89-8fe5-09ab591a276a",
   "metadata": {},
   "source": [
    "### Submit the pipeline using the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6446b6-b986-46d7-874a-e86a031ed2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = client.create_run_from_pipeline_func(\n",
    "    pipeline_func = ..., \n",
    "    namespace=..., \n",
    "    experiment_name=...,\n",
    "    run_name=f\"XGB_taxi{dt.datetime.today().isoformat()}\",\n",
    "    arguments={},\n",
    ").run_id\n",
    "print(\"Run ID: \", run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d2cd3-62bd-45ef-815d-1b0ca3e360de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercice Upgrade this pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4f457-5439-474a-95bd-c72d6477b2fa",
   "metadata": {},
   "source": [
    "#### Now we want to add :\n",
    "\n",
    "- A preprocessing component with better feature creation\n",
    "- Pipeline adjustments to be closer to a real ML workflow\n",
    "- A metric exporter in the the train phase, as in the previous notebook\n",
    "- A model push to MinIO in a separate component\n",
    "- A prediction push in a separate component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894af676-7b8f-4d22-af30-8492620d6b9e",
   "metadata": {},
   "source": [
    "#### Preprocessing component\n",
    "\n",
    "He will be responsible of \n",
    "- separate train/test from validation data\n",
    "- impute missing values on train/test\n",
    "- get rid of outliers\n",
    "- Return 2 parquet datasets : train_test that will be used by train component and validation that will be used by predict component\n",
    "\n",
    "You can debug your code by calling the function in nexts cells\n",
    "\n",
    "```python\n",
    "train_test_df, validation_df  = preprocess_data(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    input_data_path: InputPath('CSV'), \n",
    "    preprocess_train_test_data: OutputPath(),\n",
    "    preprocess_validation_data: OutputPath(),\n",
    "    label_column: int = 0,\n",
    "):\n",
    "\n",
    "    import pandas\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    # Missing values imputations with mean values\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    \n",
    "    ### load data ###\n",
    "    \n",
    "    df = pandas.read_parquet(\n",
    "        input_data_path,\n",
    "    )\n",
    "\n",
    "    ### separate train_test from validation\n",
    "    train_test = ...\n",
    "    validation_set = \n",
    "\n",
    "    # Create our imputer to replace missing values with the mean e.g.\n",
    "    imp = ...\n",
    "    imp_train = ...\n",
    "\n",
    "    # Impute our data, then transform train_test dataset \n",
    "    train_test_imp = ...\n",
    "\n",
    "    # Instanciate isolation forest to get rid of outliers\n",
    "    isolate_forest = ...\n",
    "    isolate_forest.fit(.....)\n",
    "    isolate_predictions = ...\n",
    "\n",
    "    ### clean dataset with results\n",
    "    train_test_imp = ...\n",
    "\n",
    "\n",
    "    train_test_imp.to_parquet(preprocess_train_test_data)\n",
    "    validation_set.to_parquet(preprocess_validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_component_from_func(\n",
    "    preprocess_data,\n",
    "    output_component_file='components/preprocess_data.yaml',\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=[\n",
    "        'pandas==1.0.5',\n",
    "        'scikit-learn==1.0',\n",
    "        'tensorboardX==2.5.1',\n",
    "        'pyarrow==10.0.1',\n",
    "        'fastparquet',\n",
    "        'numpy'\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c774597",
   "metadata": {},
   "source": [
    "#### Pipeline adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d64b2",
   "metadata": {},
   "source": [
    "You can see that in our first taxi_tips pipeline, we call the prediction with some data that the model already saw. This prevent from getting any usable performance metrics.\n",
    "\n",
    "lets implement a better pipeline using the 2 outputs of preprocessing component\n",
    "\n",
    "![pipeline2](./images/pipeline2.png)\n",
    "\n",
    "\n",
    "for this we need to use preprocess component outputs separately : \n",
    "\n",
    "```python\n",
    "preprocess_task = preprocess_op(input_data=get_data.output).set_memory_limit('1Gi')\n",
    "\n",
    "train_test_set = preprocess_task.outputs[\"preprocess_train_test_data\"]\n",
    "validation_set = preprocess_task.outputs[\"preprocess_validation_data\"]\n",
    "\n",
    "model_trained_on_csv = xgboost_train_on_csv_op(\n",
    "        training_data=train_test_set,\n",
    "        label_column=0,\n",
    "        objective='reg:squarederror',\n",
    "        num_iterations=200,\n",
    "    ).set_memory_limit('1Gi').outputs\n",
    "    \n",
    "xgboost_predict_on_csv_op(\n",
    "        data=validation_set,\n",
    "        model=model_trained_on_csv['model'],\n",
    "        label_column=0,\n",
    "    ).set_memory_limit('1Gi')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bc878",
   "metadata": {},
   "source": [
    "#### Metrics exporter\n",
    "\n",
    "As we exported the \"add_result\" variable in tp 1, you will use the metrics api to export training or performance test metrics \n",
    "\n",
    "```python\n",
    "\n",
    "    ### Save and exports metrics ###\n",
    "    metrics = {\n",
    "    'metrics': [{\n",
    "        'name': 'training_metric_XXXX', \n",
    "        'numberValue': .... , \n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    with open(mlpipeline_metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Add it to train and predict components!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d436b",
   "metadata": {},
   "source": [
    "#### Push model, results\n",
    "\n",
    "As in the get_data components, we will need to create a component with a minio client, and look for the right way to `put` or `fput` our object in the right `type`.\n",
    "\n",
    "\n",
    "object to export are : \n",
    "\n",
    "```python\n",
    "#####\n",
    "model_trained_on_csv[model]\n",
    "    \n",
    "xgboost_predict_on_csv_op.outputs['results']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e92068",
   "metadata": {},
   "source": [
    "Code skeletton for pickle model export: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b896552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xgboost_model_pkl(\n",
    "    bucket: str,\n",
    "    model_path: InputPath('XGBoostModel'),\n",
    "):\n",
    "    '''Make predictions using a trained XGBoost model.\n",
    "    Args:\n",
    "        bucket: Bucket name used in Minio to store the model \n",
    "        model_path: Path for the trained model in binary XGBoost format.\n",
    "    '''\n",
    "    import pickle\n",
    "    import xgboost\n",
    "    import urllib3\n",
    "    from minio import Minio\n",
    "    from datetime import datetime\n",
    "\n",
    "    # load model using input model_path\n",
    "    model =\n",
    "\n",
    "    # Create a date to store the model time & save the model as pickle\n",
    "    inference_date =\n",
    "    with open('xgboost_model_{}.pkl'.format(inference_date),'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    client = Minio(\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "    )\n",
    "\n",
    "    ### define file name to identify which object put\n",
    "    minio_model_name = 'xgboost_model_{}.pkl'.format(inference_date)\n",
    "    ### put object\n",
    "    client. ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_component_from_func(\n",
    "    save_xgboost_model_pkl,\n",
    "    output_component_file='components/save_xgboost_model_pkl.yaml',\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=[\n",
    "        'minio==6.0.2',\n",
    "        'xgboost==1.1.1',\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1db6d0c0fb62fcd92812f526c45c77dc568410c92bb0ad7cc615a53ad33175c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
